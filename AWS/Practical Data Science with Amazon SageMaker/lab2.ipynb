{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Train a Model\n",
    "\n",
    "The process of creating a machine learning (ML) model starts with data processing. After the data processing is complete, you choose an ML algorithm to train your model. The goal of model training is to create a model that you can use to make predictions with future data. Your processed data must contain a target, but your future data does not contain a target (it is unlabeled). The algorithm finds patterns in the training data that map the input data attributes to the target. The algorithm then outputs an ML model that captures these patterns. When you have a model, you can make predictions on new data that does not contain the target value.\n",
    "\n",
    "For example, if you want to train an ML model to predict if an email is spam or not spam, you would provide your model with training data that contains emails where you know the target (in this case, a label that tells whether an email is spam or not). Using this data, the algorithm creates a model that predicts if an email is spam or not spam. You can use this model to predict future email labels.\n",
    "\n",
    "In this task, you are predicting if someone has less than 50,000 USD or not. Your model is training to optimize itself so that it can predict if someone has less than 50,000 USD as accurately as possible. Model training requires some configuration, including which kind of algorithm you want to use to train. In this task, you use the XGBoost (eXtreme Gradient Boosting) algorithm. When you train a model, you also need to configure your hyperparameters. Hyperparameters are parameters that control the training job process. They can be adjusted to change various steps in the training job. Selecting the right set of hyperparameters is important in terms of model performance and accuracy. After you train the model, you evaluate the model and view the model artifacts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Set up the environment\n",
    "\n",
    "Before you start training your model, install any necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import boto3\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from IPython.display import FileLink, FileLinks\n",
    "from sagemaker import image_uris\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the dataset. In the previous lab, you exported the data files from Amazon SageMaker Data Wrangler to an Amazon Simple Storage Service (Amazon S3) bucket. You split the dataset into training (70 percent), validation (20 percent), and test (10 percent) datasets. The training and validation datasets are used during training. The test dataset is used in model evaluation after deployment.\n",
    "\n",
    "To train using Amazon SageMaker, the datasets must be in either libSVM or CSV format. This lab uses the CSV format for training. \n",
    "\n",
    "To view the dataset files that you created in the previous lab, follow these steps below:\n",
    "\n",
    "1. Choose the bucket icon from the left menu bar.\n",
    "\n",
    "1. In the list of buckets, choose the Amazon S3 bucket that contains **labdatabucket** in its name.\n",
    "\n",
    "Opening the .csv files opens new tabs in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **lab_2.ipynb** tab to the side or choose the **lab_2.ipynb** tab, and then from the toolbar, select **File** and **New View for Notebook**. You can now have the directions displayed as you explore the .csv files.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions. When you are finished exploring the .csv files, return to the notebook by choosing the **lab_2.ipynb** tab.\n",
    "\n",
    "1. Choose (double-click) the **scripts** folder, choose (double-click) the **data** folder, choose (double-click) the **train** folder, and then choose (double-click) the **adult_data_processed_train.csv** file to view its contents.\n",
    "\n",
    "1. In the left pane, choose **data** from the <i class=\"fas fa-folder\" style=\"color:white\"></i> **/ ... /data/train/** breadcrumbs link.\n",
    "\n",
    "1. Choose (double-click) the **validation** folder, and then choose (double-click) the **adult_data_processed_validation.csv** file to view its contents.\n",
    "\n",
    "You have viewed the dataset files. Now, configure the training and validation paths that your training job uses as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)\n",
    "prefix = 'scripts/data'\n",
    "output_path = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "# Configure the training paths\n",
    "train_path = f\"s3://{bucket}/{prefix}/train/adult_data_processed_train.csv\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation/adult_data_processed_validation.csv\"\n",
    "\n",
    "# Set up the TrainingInput objects\n",
    "train_input = TrainingInput(train_path, content_type='text/csv')\n",
    "validation_input = TrainingInput(validation_path, content_type='text/csv')\n",
    "\n",
    "# Print the training and validation paths\n",
    "print(f'Training path: {train_path}')\n",
    "print(f'Validation path: {validation_path}')\n",
    "\n",
    "# Set the container, name, and tags\n",
    "create_date = strftime(\"%m%d%H%M\")\n",
    "container = image_uris.retrieve(framework='xgboost',region=boto3.Session().region_name,version='1.2-1')\n",
    "run_name = 'lab-2-run-{}'.format(create_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Configure an estimator object\n",
    "\n",
    "An estimator is a high level interface for SageMaker training. You create an estimator object by supplying the required parameters, such as AWS Identity and Access Management (IAM) role, compute instance count and type, and the Amazon S3 output path. This lab uses the XGBoost built-in algorithm for the SageMaker generic estimator. XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in handling a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems. In this case, you are using XGBoost to solve a classification problem (whether someone is making less than 50,000 USD or not).\n",
    "\n",
    "In this lab you create an XGBoost estimator by using the *sagemaker.estimator.Estimator* class. In the following example code, the XGBoost estimator is named *xgb_model*. To construct the SageMaker estimator, specify the following parameters:\n",
    "\n",
    "- **image_uri**: The training container image URI. In this example, the SageMaker XGBoost training container URI is specified using *image_uris.retrieve*.\n",
    "- **role**: The IAM role that SageMaker uses to perform tasks on your behalf (for example, reading training results, calling model artifacts from Amazon S3, and writing training results to Amazon S3). \n",
    "- **instance_count and instance_type**: The type and number of Amazon EC2 ML compute instances to use for model training. For this lab, you use a single ml.m5.xlarge instance, which has 4 CPUs, 16 GB of memory, an Amazon Elastic Block Store (Amazon EBS) storage, and a high network performance.\n",
    "- **output_path**: The path to the S3 bucket where SageMaker stores the model artifact and training results.\n",
    "- **sagemaker_session**: The session object that manages interactions with SageMaker API operations and other AWS service that the training job uses.\n",
    "- **rules**: A list of Amazon SageMaker Debugger built-in rules. In this example, the create_xgboost_report() rule creates an XGBoost report that provides insights into the training progress and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = sagemaker.estimator.Estimator(\n",
    "    image_uri = container,\n",
    "    role = role, \n",
    "    instance_count = 1, \n",
    "    instance_type ='ml.m5.xlarge',\n",
    "    output_path = output_path,\n",
    "    sagemaker_session = sagemaker_session,\n",
    "    rules=[\n",
    "        Rule.sagemaker(rule_configs.create_xgboost_report())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Configure hyperparameters\n",
    "\n",
    "Hyperparameters directly control model structure, function, and performance. Hyperparameter tuning allows data scientists to tweak model performance for optimal results. This process is an essential part of machine learning, and choosing appropriate hyperparameter values is crucial for success.\n",
    "\n",
    "You can set hyperparameters for the XGBoost algorithm by calling the *set_hyperparameters* method of the estimator.\n",
    "\n",
    "Refer to [XGBoost Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html) for more information about XGBoost hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.set_hyperparameters(\n",
    "    max_depth = 5,\n",
    "    eta = 0.2,\n",
    "    gamma = 4,\n",
    "    min_child_weight = 6,\n",
    "    subsample = 0.7,\n",
    "    verbosity = 0,\n",
    "    objective = 'binary:logistic',\n",
    "    num_round = 800\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Run a SageMaker training job\n",
    "\n",
    "Now that you have configured your estimator object and hyperparameters, you are ready to start training the model. The fit() method starts the training script. To start model training, call the estimator's fit() method with the training and validation datasets. If you set `wait=True`, the fit() method displays progress logs and waits until training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(\n",
    "    {\n",
    "        \"train\": train_input,\n",
    "        \"validation\": validation_input\n",
    "    },\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-clipboard-check\" style=\"color:#18ab4b\"></i> **Expected output:** If the estimator and hyperparameter configuration are correct and the training job is started correctly, you should see the following output:\n",
    "\n",
    "```plain\n",
    "************************\n",
    "**** EXAMPLE OUTPUT ****\n",
    "************************\n",
    "\n",
    "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-08-09-20-09-56-628\n",
    "2023-08-09 20:09:56 Starting - Starting the training job...\n",
    "2023-08-09 20:10:19 Starting - Preparing the instances for trainingCreateXgboostReport: InProgress\n",
    "......\n",
    "2023-08-09 20:11:21 Downloading - Downloading input data...\n",
    "2023-08-09 20:11:55 Training - Downloading the training image...\n",
    "2023-08-09 20:12:20 Training - Training image download completed. Training in progress....\n",
    "2023-08-09 20:12:56 Uploading - Uploading generated training model...\n",
    "2023-08-09 20:13:20 Completed - Training job completed\n",
    "..Training seconds: 107\n",
    "Billable seconds: 107\n",
    "```\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** The training takes approximately 3–4 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Evaluate a model\n",
    "\n",
    "After the training job has completed, you can download an XGBoost training report generated by SageMaker Debugger. The XGBoost training report offers you insights into the training progress and results, such as the loss function with respect to iteration, feature importance, confusion matrix, accuracy curves, and other statistical results of training. \n",
    "\n",
    "For SageMaker XGBoost training jobs, use the Debugger `CreateXgboostReport` rule to receive a comprehensive training report of the training progress and results.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** It can take approximately 4–8 minutes to finalize the report. In the following code block, *time.sleep(300)* is used to wait 5 minutes for the report to finish creating. If the report link does not appear in the second cell, run the code cells again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "time.sleep(300)\n",
    "rule_output_path = xgb_model.output_path + \"/\" + xgb_model.latest_training_job.job_name + \"/rule-output\"\n",
    "! aws s3 ls {rule_output_path} --recursive\n",
    "! aws s3 cp {rule_output_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link in the output of the next cell opens a new tab in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **lab_2.ipynb** tab to the side or choose the **lab_2.ipynb** tab, and then from the toolbar, select **File** and **New View for Notebook**. You can now have the directions displayed as you explore the XGBoost report.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions. When you are finished exploring the XGBoost report, return to the notebook by choosing the **lab_2.ipynb** tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Click link below to view the XGBoost Training report\", FileLink(\"CreateXgboostReport/xgboost_report.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** After you run this code, you should see the following output: **'Click link below to view the XGBoost Training report' <span style=\"ssb_sm_blue\">CreateXgboostReport/xgboost_report.html</span>**\n",
    "\n",
    "Choose the link and scroll down until you make it to the **Confusion Matrix**. The confusion matrix illustrates in a table the number of correct and incorrect predictions for each class by comparing an observation's predicted class and its true class. When you go to the diagram you see **true positive (TP)**, **true negative (TN)**, **false positive (FP)**, and **false negative (FN)** values.\n",
    "\n",
    "- **True positive:** If the actual classification is positive and the predicted classification is positive (1,1), this is called a **true positive (TP)** result because the positive sample was correctly identified by the classifier. \n",
    "- **False negative:** If the actual classification is positive and the predicted classification is negative (1,0), this is called a **false negative (FN)** result because the positive sample is incorrectly identified by the classifier as being negative. \n",
    "- **False positive:** If the actual classification is negative and the predicted classification is positive (0,1), this is called a **false positive (FP)** result because the negative sample is incorrectly identified by the classifier as being positive. \n",
    "- **True negative**: If the actual classification is negative and the predicted classification is negative (0,0), this is called a **true negative (TN)** result because the negative sample gets correctly identified by the classifier.\n",
    "\n",
    "Next, scroll down to **Evaluation of the Confusion Matrix** and take a closer look at the **Classification report** to understand the summary of the precision, recall, and F1-score for each class.\n",
    "\n",
    "- **Precision**: Measures the fraction of actual positives that were predicted as positives out of all of those predicted as positive. The range is 0 to 1, and a larger value indicates better accuracy. Precision expresses the proportion of the data points that your model says was relevant and that were actually relevant. Precision is a good measure to consider, especially when the costs of FP are high.\n",
    "- **Recall/Sensitivity/True Positive Rate (TPR)**: Measures the fraction of actual positives that were predicted as positives. The range is also 0 to 1, and a larger value indicates a better predictive accuracy. This is also known as Recall/Sensitivity. This measure expresses the ability to find all the relevant instances in a dataset.\n",
    "- **F1-Score**: Demonstrates your target metric, which is the harmonic mean of precision and recall. F1 takes both FP and FN into account to give the same weight to precision and recall.\n",
    "\n",
    "You are trying to predict if people make less than 50,000 USD so you can promote government assistance services to qualified citizens. In this case, the F1-Score is a good measure to use because it takes FP (people who make over 50,000 USD who were labeled as making less than 50,000 USD) and FN (people who make under 50,000 USD who were labeled as making more than 50,000 USD) into account. You want to make sure that your precision and recall are both high, and the F1-Score takes both measures into account. In the next lab, you optimize the model by tuning the hyperparameters to see if you can get a higher F1-Score.\n",
    "\n",
    "What are the **Precision**, **Recall**, **F1-Score**, and **Overall Accuracy** for this model? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: View the model artifacts\n",
    "\n",
    "SageMaker stores the model artifact in your S3 bucket. To find the location of the model artifact, follow these steps:\n",
    "\n",
    "1. Choose the bucket icon from the left menu bar.\n",
    "\n",
    "1. In the list of buckets, open the Amazon S3 bucket that contains **labdatabucket** in its name.\n",
    "\n",
    "1. Navigate to the **scripts/data/output/ sagemaker-xgboost-.../output** subfolder. \n",
    "\n",
    "You see the model artifact **model.tar.gz** in the subfolder. This is the model that you created with your SageMaker Estimator by calling the fit() method.\n",
    "\n",
    "You viewed the model artifacts, including the model.tar.gz file. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
