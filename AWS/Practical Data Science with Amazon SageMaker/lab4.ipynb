{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Deploy a model for real-time inference\n",
    "\n",
    "After you build, train, and evaluate your machine learning (ML) model to ensure that it’s solving the intended business problem proposed, you want to deploy that model to facilitate decision-making in business operations. Amazon SageMaker offers a broad range of deployment options that vary from low latency and high throughput to long-running inference jobs. With SageMaker Inference, you can either set up an endpoint that returns inferences or run batch inferences from your model.\n",
    "\n",
    "SageMaker provides multiple inference options so that you can pick the option that best suits your workload. For this lab you use real-time inference. In particular, real-time inference is a great option for hosting your models when you have low and consistent latency and throughput-sensitive workloads. Use real-time inference for a persistent and fully managed endpoint (REST API) that can handle sustained traffic, backed by the instance type of your choice. Real-time inference can support payload sizes up to 6 MB and processing times of 60 seconds.\n",
    "\n",
    "To prepare the model for deployment, you define the endpoint configuration where you specify the name of one or more models in production (variants) and the ML compute instances that you want SageMaker to launch to host each production variant. When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, SageMaker launches them in multiple Availability Zones. This ensures continuous availability. SageMaker manages deployment of the instances. When you have your model and endpoint configuration, use the [CreateEndpoint API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpoint.html) to create your endpoint. Provide the endpoint configuration to SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration. \n",
    "\n",
    "In this task, you deploy a model, create an endpoint, and run real-time inference on the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Set up the environment\n",
    "\n",
    "Before you start tuning your model, install any necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies \n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "sm_runtime = sess.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the dataset \n",
    "s3 = boto3.resource('s3')\n",
    "for buckets in s3.buckets.all():\n",
    "    if 'labdatabucket' in buckets.name:\n",
    "        bucket = buckets.name\n",
    "print(\"Bucket: \", bucket)\n",
    "prefix = 'scripts/data'\n",
    "\n",
    "# Download the file to use for inference later in the notebook\n",
    "s3_client.download_file(bucket, f'{prefix}/test/adult_data_processed_test.csv', 'adult_data_processed_test.csv')\n",
    "\n",
    "# Open adult_data_processed_test.csv with pandas, save the first column as df_labels, remove the labels from the dataset, and save it back to adult_data_processed_test.csv\n",
    "df = pd.read_csv('adult_data_processed_test.csv')\n",
    "df_labels = df.iloc[:, 0]\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df.to_csv('adult_data_processed_test_no_target.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import the best model that you selected from the previous lab. The **model.tar.gz** file was uploaded during the lab environment creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the model to your Amazon S3 bucket\n",
    "s3_client.upload_file(\n",
    "    Filename=\"model.tar.gz\", Bucket=bucket, Key=f\"{prefix}/models/model.tar.gz\"\n",
    ")\n",
    "\n",
    "# Set a date to use in the model name\n",
    "create_date = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "model_name = 'income-model-{}'.format(create_date)\n",
    "\n",
    "# Retrieve the container image\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    region=region, \n",
    "    framework='xgboost', \n",
    "    version='1.5-1'\n",
    ")\n",
    "\n",
    "# Set up the model\n",
    "income_model = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': f's3://{bucket}/{prefix}/models/model.tar.gz',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Configure an endpoint\n",
    "\n",
    "You have already created a model in the previous lab. You are now ready to create an endpoint configuration. \n",
    "\n",
    "Set up the endpoint configuration name and the instance type that you want to use.\n",
    "\n",
    "To create an endpoint configuration, you need to set the following options:\n",
    "- **VariantName**: The name of the production variant (one or more models in production).\n",
    "- **ModelName**: The name of the model that you want to host. This is the name that you specified when you created the model.\n",
    "- **InstanceType**: The compute instance type.\n",
    "- **InitialInstanceCount**: The number of instances to launch initially.\n",
    "\n",
    "To log the inputs to your endpoint and the inference outputs from SageMaker real-time endpoints to Amazon S3, you can enable a feature called Data Capture. Data Capture is commonly used to record information that can be used for training, debugging, and monitoring. When you explore your endpoint in Amazon SageMaker Studio, more details about the endpoint will be displayed when Data Capture is enabled. The configuration for Data Capture features later in this lab to show you how to enable it.\n",
    "\n",
    "Refer to [Capture Data](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html) for more information about adding Data Capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an endpoint config name. Here you create one based on the date  \n",
    "# so it you can search endpoints based on creation time.\n",
    "endpoint_config_name = 'income-model-real-time-endpoint-{}'.format(create_date)                              \n",
    "instance_type = 'ml.m5.xlarge'   \n",
    "initial_sampling_percentage = 25 # Choose a value between 0 and 100\n",
    "capture_modes = [ \"Input\",  \"Output\" ] # Specify input, output, or both\n",
    "\n",
    "endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name, # You will specify this name in a CreateEndpoint request.\n",
    "    # List of ProductionVariant objects, one for each model that you want to host at this endpoint.\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\", # The name of the production variant.\n",
    "            \"ModelName\": model_name, \n",
    "            \"InstanceType\": instance_type, # Specify the compute instance type.\n",
    "            \"InitialInstanceCount\": 1 # Number of instances to launch initially.\n",
    "        }\n",
    "    ],\n",
    "    DataCaptureConfig= {\n",
    "        'EnableCapture': True, # Whether data should be captured or not.\n",
    "        'InitialSamplingPercentage' : initial_sampling_percentage,\n",
    "        'DestinationS3Uri': f's3://{bucket}/data-capture',\n",
    "        'CaptureOptions': [{\"CaptureMode\" : capture_mode} for capture_mode in capture_modes]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created EndpointConfig: {endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Create an endpoint\n",
    "\n",
    "Next, create an endpoint. When you create a real-time endpoint, SageMaker launches the ML compute instances and deploys one or more models as specified in the configuration. In this lab, you are only deploying one model for inference. In SageMaker, you can create a multi-model endpoint. Refer to [Invoke a Multi-Model Endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html) for more information about multi-model endpoints.\n",
    "\n",
    "When the endpoint is in service, the helper function will print the endpoint Amazon Resource Name (ARN). Endpoint creation will take approximately 3–7 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the endpoint\n",
    "# Set the name of the endpoint. The name must be unique within an AWS Region in your AWS account.\n",
    "endpoint_name = '{}-name'.format(endpoint_config_name)\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, \n",
    "    EndpointConfigName=endpoint_config_name\n",
    ") \n",
    "\n",
    "def wait_for_endpoint_creation_complete(endpoint):\n",
    "    \"\"\"Helper function to wait for the completion of creating an endpoint\"\"\"\n",
    "    response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response.get(\"EndpointStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Endpoint Creation\")\n",
    "        time.sleep(15)\n",
    "        response = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = response.get(\"EndpointStatus\")\n",
    "\n",
    "    if status != \"InService\":\n",
    "        print(f\"Failed to create endpoint, response: {response}\")\n",
    "        failureReason = response.get(\"FailureReason\", \"\")\n",
    "        raise SystemExit(\n",
    "            f\"Failed to create endpoint {create_endpoint_response['EndpointArn']}, status: {status}, reason: {failureReason}\"\n",
    "        )\n",
    "    print(f\"Endpoint {create_endpoint_response['EndpointArn']} successfully created.\")\n",
    "\n",
    "wait_for_endpoint_creation_complete(endpoint=create_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker Studio, you can review the endpoint details under the **Endpoints** tab.\n",
    "\n",
    "The next step opens a new tab in SageMaker Studio. To follow these directions, use one of the following options:\n",
    "- **Option 1:** View the tabs side by side. To create a split screen view from the main SageMaker Studio window, either drag the **real_time_inference.ipynb** tab to the side or choose the **real_time_inference.ipynb** tab, and then from the toolbar, select **File** and **New View for Notebook**. You can now have the directions visible as you explore the endpoint.\n",
    "- **Option 2:** Switch between the SageMaker Studio tabs to follow these instructions. When you are finished exploring the endpoint, return to the notebook by choosing the **real_time_inference.ipynb** tab.\n",
    "\n",
    "1. Choose the **SageMaker Home** icon.\n",
    "2. Choose **Deployments**.\n",
    "3. Choose **Endpoints**.\n",
    "\n",
    "SageMaker Studio displays the **Endpoints** tab.\n",
    "\n",
    "4. Select the endpoint that has **income-model-real-time-** in the **Name** column.\n",
    "\n",
    "If the endpoint does not appear, choose the refresh icon until the endpoint appears in the list.\n",
    "\n",
    "SageMaker Studio displays the **ENDPOINT DETAILS** tab.\n",
    "\n",
    "5. Choose the **AWS settings** tab.\n",
    "\n",
    "If you opened the endpoint before it finished creating, choose the refresh icon until the **Endpoint status** changes from *Creating* to *InService*.\n",
    "\n",
    "The **Endpoint type** is listed as **Real-time**. The **Data capture settings** and **Endpoint configuration settings** sections show the configurations that you chose earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4: Test an endpoint and generate a prediction\n",
    "\n",
    "After you deploy your model by using SageMaker hosting services, you can test your model on that endpoint by sending it test data.\n",
    "\n",
    "You have several customer records that you know show an income less than 50,000 USD (an **income** value of **1**), and several that have an income greater than or equal to 50,000 USD (an **income** value of **0**). Invoke the endpoint with these records and view the returned scores.\n",
    "\n",
    "To view real-time predictions from the endpoint, you read the returned body text from the response, which contains a list of the prediction scores. The score for each record ranges from **0** to **1**, with numbers closer to **1** indicating that those customers are more likely to have an income less than 50,000 USD. For example, a customer with a prediction score of **0.42** is more likely to have an income less than 50,000 USD than a customer with a prediction score of **0.14**. To calculate precision, recall, and the F1 scores, the values are rounded to 0 or 1 and compared to the target data labels that you saved earlier in the notebook.\n",
    "\n",
    "<i class=\"fas fa-sticky-note\" style=\"color:#ff6633\"></i> **Note:** There are over 5,000 records in the **adult_data_processed_test_no_target.csv** dataset. It takes 2–3 minutes to complete all of the real-time inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get-model-A-predictions\n",
    "predictions = \"\"\n",
    "\n",
    "# set the cutoff value for the binary classification problem\n",
    "cutoff_value = 0.5\n",
    "\n",
    "# determine the classification based on a probability value \n",
    "def convert_probability_to_binary(probability):\n",
    "    if probability >= cutoff_value:\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "with open(\"adult_data_processed_test_no_target.csv\", \"r\") as f:\n",
    "    for row in f:\n",
    "        # print(\".\", end=\"\", flush=True)\n",
    "        payload = row.rstrip(\"\\n\")\n",
    "        response = sm_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"text/csv\",\n",
    "            Body=payload\n",
    "        )\n",
    "\n",
    "        pred_probability = response[\"Body\"].read().decode(\"utf-8\")\n",
    "        new_prediction = convert_probability_to_binary(float(pred_probability))\n",
    "        predictions = \",\".join([predictions, new_prediction])\n",
    "f.close()\n",
    "\n",
    "# Convert the predictions to a numpy array and round the values (values closer to 0 round to 0, while values closer to 1 round to 1)\n",
    "pred_np = np.fromstring(predictions[1:], sep=\",\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.5: Evaluate the prediction power with a confusion matrix\n",
    "\n",
    "Because ML models are approximations of the real world, some of the model's predictions are likely in error. In some applications all types of prediction errors are truly equal in impact. In other applications, one kind of error can be much more costly or consequential than another, measured in absolute or relative terms, in dollars, time, or something else. In ML, the accuracy of the model is defined as the number of correct predictions divided over the total number of predictions. In this lab you are trying to predict if people make less than 50,000 USD so that you can promote government assistance services to qualified citizens.\n",
    "\n",
    "The confusion matrix compares the predicted and actual values. It illustrates in a table the number of correct and incorrect predictions for each class by comparing an observation's predicted class and its true class. When you run the code cell, you see **true positive (TP)**, **true negative (TN)**, **false positive (FP)**, and **false negative (FN)** values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#evaluate-prediction-power\n",
    "# Configure the metrics for a confusion matrix with the original labels and the predictions\n",
    "cf_matrix = confusion_matrix(df_labels, pred_np)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                        cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "            zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "# Display the confusion matrix\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you evaluate inference results, there are several measures that you can use to see how effective and accurate your model is. The most common measures are **accuracy**, **precision**, **recall**, and **F1 score**. \n",
    "\n",
    "- **Accuracy**: A measure calculated by dividing the sum of True Positives and True Negatives by the sum of Positives and Negatives. The calculation is (TP + TN) / (P + N). \n",
    "- **Precision**: A measure calculated by dividing the True Positives by the sum of True Positives and False Positives. The calculation is TP / (TP + FP).\n",
    "- **Recall**: A measure calculated by dividing the the True Positives by the sum of True Positives and False Negatives. The calculation is TP / (TP + FN).\n",
    "- **F1 score**: A measure calculated by multiplying prevision and recall, and then dividing that number by the sum of precision and recall. Then, the result is multiplied by 2. The calculation is 2 * ((precision * recall)/(precision + recall)).\n",
    "\n",
    "For this dataset, you want high *precision*. The advocacy group does not have enough funds to provide support for everyone who makes less than 50,000 USD. They want to maximize their impact by correctly identifying, with high *precision*, individuals who make less than 50,000 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the F1 score\n",
    "print('You can view a full classification report below:\\n')\n",
    "\n",
    "# Calculate the accuracy, precision, and recall of df_labels and predictions using sklearn\n",
    "print(classification_report(df_labels, pred_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the **precision** value listed next to the line starting with **1**? Is **recall** higher or lower than **precision**? \n",
    "\n",
    "If you want to tune your model, your goal is to continue increasing precision, even if it means that your recall results drop. The trade off between precision and recall is a common ML problem. The F1 score helps weigh the trade-off for models and use cases that are working to maximize the F1 score. For this use case, it is fine if the F1 score and the recall both drop, as long as precision is increased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.6: Delete the endpoint\n",
    "\n",
    "Cleaning up an endpoint can be accomplished in three steps. First, delete the endpoint. Then delete the endpoint configuration. Finally, if you no longer need the model that you deployed, delete the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete-resources\n",
    "# Delete endpoint\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete endpoint configuration\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "                   \n",
    "# Delete model\n",
    "sm.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab session and continue with the **Conclusion**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "741de909edea0d5644898c592544ed98bede62b404d20772e5c4abc3c2f12566"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
